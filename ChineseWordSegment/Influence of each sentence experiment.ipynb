{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence and Distribution of different sentences\n",
    "\n",
    "In this part of experiment, we look inside the distribution of different sentences with their distance of text. \n",
    "\n",
    "Because if we only consider the one segment in a sentence, we could not get the right `main sentences` usually, we need to find a more general approach to achieve finding the `main sentences` from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from text_summa\n",
    "\n",
    "ry import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utlis.get_word_vector_by_glove import get_consistent\n",
    "from utlis.get_word_vector_by_glove import get_local_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_corelation(corelation, sub_plot=None):\n",
    "    mean = np.mean(corelation)\n",
    "    _1st_percentile = np.percentile(corelation, 25)\n",
    "    _3st_percentile = np.percentile(corelation, 75)\n",
    "    \n",
    "    if sub_plot:\n",
    "        plt.subplot(*sub_plot)\n",
    "    \n",
    "    print('corelation length is: {}'.format(len(corelation)))\n",
    "    plt.plot(range(len(corelation)), corelation, c=(0, 0, 0.2))\n",
    "    plt.fill_between(range(len(corelation)), corelation)\n",
    "    plt.plot([mean] * len(corelation), 'r--')\n",
    "    plt.plot([_1st_percentile] * len(corelation), 'g+')\n",
    "    plt.plot([_3st_percentile] * len(corelation), 'b*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是一篇广告，不适合做摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is the distance of first half of ads and tail half of ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_two_sentence_distance(\"\".join(words[: len(words)//2]), \"\".join(words[len(words)//2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_verbose(corelations):\n",
    "    threshold = 0.03\n",
    "    variance = np.var(corelations)\n",
    "    return (1/variance) * np.log(len(corelations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x, old_min=0, old_max=10):\n",
    "    new_x = (x - old_min) / (old_max - old_min) * 100\n",
    "    return new_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coherence(sentences):\n",
    "    threshold = 0.5\n",
    "    text = \"\".join(sentences)\n",
    "    half_head = text[:len(text)//2]\n",
    "    half_tail = text[len(text)//2:]\n",
    "    distance = get_two_sentence_distance(half_head, half_tail)\n",
    "    print('distance: {}'.format(distance))\n",
    "    coherence = (1/(distance)) * np.log(len(sentences))\n",
    "    return coherence\n",
    "\n",
    "def is_coherent(text):\n",
    "    coherence = get_coherence(text)\n",
    "    threshold = 0.03\n",
    "    return coherence > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(array):\n",
    "    array = np.array(array)\n",
    "    array -= np.max(array, axis=0)\n",
    "    return np.exp(array) / sum(np.exp(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_text_corelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-49d6667b3b79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'experiment/test_text.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorelations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_text_corelation' is not defined"
     ]
    }
   ],
   "source": [
    "f = 'experiment/test_text.txt'\n",
    "corelations = get_text_corelation(f)\n",
    "plot_corelation(softmax(corelations[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the distance of Title with each sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corelations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e68476deddd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorelations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corelations' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = [s for s, d in corelations[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = '早高峰无盖井卡住两辆车 居民盼相关部门解决'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_title_distance(title, sentences):\n",
    "    return get_text_sentences_distances(title, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b62cda2f70ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_title_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "dis = get_title_distance(title, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-60d883f80280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitle_corelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dis' is not defined"
     ]
    }
   ],
   "source": [
    "title_corelations = softmax([1 - d for _ , d in dis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title_corelations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a6dcc18c56d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_corelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'title_corelations' is not defined"
     ]
    }
   ],
   "source": [
    "plot_corelation(title_corelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corelations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c53077edbb4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorelations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corelations' is not defined"
     ]
    }
   ],
   "source": [
    "plot_corelation(softmax(corelations[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The corelation with all the text and title is very similar, at least for \"experiment/test_text.txt\" this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The furthure test. \n",
    "\n",
    "In order to get the more general information, we need to test more texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test of bounch texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['experiment/ads.txt', 'experiment/test_text.txt', 'experiment/test_text_01.txt', 'experiment/test_text_02.txt', 'experiment/test_text_03.txt', 'experiment/test_text_04.txt', 'experiment/text_06.txt', 'experiment/text_07.txt', \\\n",
    "         'experiment/many_verbose.txt', 'experiment/not_coherent.txt', 'experiment/test_wechat.txt']\n",
    "titles = ['再不去闯，梦想永远只是梦想', '早高峰无盖井卡住两辆车 居民盼相关部门解决', '使馆区武警目击作案过程 围堵拦截小偷', '七国集团峰会联合公报涉及东海、南海问题 中国外交部回应 快看', '端午小长假高速不免费 9个收费站可用支付宝缴费', '轻松一刻：全班15对情侣，这才是最虐狗的班级', \\\n",
    "         '山东省检察院派员出庭于欢故意伤害案二审法庭（出庭意见书全文）', '除了四川人民 最爱打麻将的竟是土耳其', '处女座剧组人设崩塌？穿帮+广告毁了《欢乐颂2', '比亚迪强大中国车王朝擂台赛杭州站开赛', \\\n",
    "         '这个国家每年“六一”前都会给老百姓发钱去度假']\n",
    "assert len(files) == len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_one(f, t, index = 0, f_number=1, fig=None):\n",
    "    fig = fig or plt.figure(figsize=(20, 8))\n",
    "    print(\"{}, {}\".format(f, t))\n",
    "    \n",
    "    corelations = get_text_corelation(f)\n",
    "    sentences = [s for s, d in corelations[2]]\n",
    "    coherence = get_coherence(sentences)\n",
    "    print('coherence == {}'.format(coherence))\n",
    "\n",
    "    fig.add_subplot(f_number, 3, index * 3 + 1)\n",
    "    plot_corelation(softmax(corelations[1]))\n",
    "    title_distance = get_title_distance(t, sentences)\n",
    "    title_corelation = [1 - d for _, d in title_distance]\n",
    "    fig.add_subplot(f_number, 3, index * 3 + 2)\n",
    "    plot_corelation(softmax(title_corelation))\n",
    "    \n",
    "    fig.add_subplot(f_number, 3, index * 3 + 3)\n",
    "    plot_corelation(softmax(corelations[1]))\n",
    "    plot_corelation(softmax(title_corelation))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_mutiply_f_t(f_t):\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    index = 0\n",
    "    for f, t in f_t:\n",
    "        plot_one(f, t, index, f_number=len(f_t), fig=fig)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_t = list(zip(files, titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_one(*f_t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Accumulate Corelation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accumulate(x):\n",
    "    x = np.array(x)\n",
    "    acc = [np.sum(x[:(index+1)]) for index in range(len(x))]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulate([1, 2, 3]) == [1, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_complex_corelation(title_corelations, content_corelations):\n",
    "    def f(title_corelation, content_corelation):\n",
    "        p = 0.5\n",
    "        return p * title_corelation + (1 - p) * content_corelation\n",
    "    corelations = []\n",
    "    for t_c, c_c in zip(title_corelations, content_corelations):\n",
    "        corelations.append(f(t_c, c_c))\n",
    "    return corelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_one_file_complex_corelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1c115bddda66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'experiment/text_07.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcomplex_corelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_one_file_complex_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'most trivial '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_one_file_complex_corelation' is not defined"
     ]
    }
   ],
   "source": [
    "file_index = files.index('experiment/text_07.txt')\n",
    "complex_corelation = get_one_file_complex_corelation(files[file_index], titles[file_index])\n",
    "sentences = get_text_sentence(files[file_index])\n",
    "\n",
    "print('most trivial ')\n",
    "for index, s in enumerate(sentences):\n",
    "    if complex_corelation[index] < np.percentile(complex_corelation, 25):\n",
    "        print(s)\n",
    "        \n",
    "print('most important')\n",
    "for index, s in enumerate(sentences):\n",
    "    if complex_corelation[index] > np.percentile(complex_corelation, 75):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_outliner(x, array):\n",
    "    _1st_percentile = np.percentile(array, 25)\n",
    "    _3th_percentile = np.percentile(array, 75)\n",
    "    threshold = 1.5\n",
    "    if (_1st_percentile / x) > threshold or (x / _3th_percentile) > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_outliner(Xs):\n",
    "    Xs = np.array(Xs)\n",
    "    Xs = list(filter(lambda x: not is_outliner(x, Xs), Xs))\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 12, 4, 14, -1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_outliner([1, 2, 1, 43, 12, 4, 14, -1, 999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_2_loss(y_hats, ys):\n",
    "    def f(x):\n",
    "        max_length = 1000\n",
    "        if x > max_length: return max_length\n",
    "        else: return x\n",
    "    return np.sum(np.square(y_hats - ys))  * 1 / f(len(y_hats))\n",
    "\n",
    "def have_main_point(complex_corelation, plot=True):\n",
    "    if len(complex_corelation) < 10: \n",
    "        return False, -1\n",
    "    else:\n",
    "#        print('before clean length is: {}'.format(len(complex_corelation)))\n",
    "        complex_corelation = clean_outliner(complex_corelation)\n",
    "#        print('end clean legnth is: {}'.format(len(complex_corelation)))\n",
    "        k = (1.0 - 0) / (len(complex_corelation) - 0)\n",
    "        ys = k * np.arange(0, len(complex_corelation))\n",
    "        acc = accumulate(complex_corelation)\n",
    "        if plot: plot_corelation(acc)    \n",
    "        l2_loss = l_2_loss(acc, ys)\n",
    "        threshold = 5.0e-4\n",
    "        logging.info(l2_loss)\n",
    "#        print(\"l2-loss: {}\".format(l2_loss))\n",
    "        if l2_loss < threshold:\n",
    "            return False, l2_loss\n",
    "        else:\n",
    "            return True, l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_to_summary():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_if_one_file_fit_summary(text, title):\n",
    "    mini_length = 200\n",
    "    if not os.path.isfile(text) and len(text) < mini_length: \n",
    "        return False, -1\n",
    "    else:\n",
    "        complex_corelation = get_one_file_complex_corelation(text, title)\n",
    "        return have_main_point(complex_corelation, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distinct(array):\n",
    "    reversed_array = array[::-1]\n",
    "    for index, e in enumerate(reversed_array[:-1]):\n",
    "        if e in reversed_array[index+1:]: array[-(1+index)] = None\n",
    "    array  = [e for e in array if e is not None]\n",
    "    return array\n",
    "\n",
    "def get_main_sentene_with_theme_corelation(sentences, corelations):\n",
    "    print(sorted(zip(sentences, corelations), key=lambda x: x[1], reverse=True))\n",
    "    important = clean_outliner(corelations)\n",
    "    avg_length = np.mean([len(sentence) for sentence in sentences])\n",
    "    max_sentences_num = int(200/avg_length)\n",
    "    important = distinct(important)\n",
    "    most_important = np.sort(important)[::-1][:max_sentences_num]\n",
    "    most_important_sentences = distinct([s for i, s in enumerate(sentences) if corelations[i] in most_important])\n",
    "    print(most_important)\n",
    "    return most_important_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def in_same_sentence(subsentence1, subsentence2, text):\n",
    "    begin_index = text.index(subsentence1)\n",
    "    if find_complete_sentence(subsentence1, text[begin_index:]) == find_complete_sentence(subsentence2, text[begin_index:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def get_complete_sentences_with_corelations(sentences, single_subsentence_corelations, text):\n",
    "    complete_sentences = []\n",
    "    complete_sentences_corelations = []\n",
    "\n",
    "    corelations = [None if is_outliner(x, single_subsentence_corelations) else x for x in single_subsentence_corelations]\n",
    "    single_complete_corelations = [corelations[0]]\n",
    "    single_complete_sentence = [sentences[0]]\n",
    "    \n",
    "    for index in range(1, len(sentences)):\n",
    "        sub = sentences[index]\n",
    "        last_word = single_complete_sentence[-1]\n",
    "        if in_same_sentence(last_word, sub, text):\n",
    "            single_complete_sentence.append(sub)\n",
    "            single_complete_corelations.append(corelations[index])\n",
    "        else:\n",
    "            complete_sentences.append(single_complete_sentence)\n",
    "            complete_sentences_corelations.append(single_complete_corelations)\n",
    "            \n",
    "            single_complete_sentence = [sub]\n",
    "            single_complete_corelations = [corelations[index]]\n",
    "            \n",
    "        if index == len(sentences) - 1:\n",
    "            complete_sentences.append(single_complete_sentence)\n",
    "            complete_sentences_corelations.append(single_complete_corelations)\n",
    "    \n",
    "    return zip(complete_sentences, complete_sentences_corelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_summary_with_fit_summary_test(text, title):\n",
    "    ## get the main sentences of one text\n",
    "    ## if no main sentene or not fit to get summary, return None else return sentences\n",
    "    mini_length = 250\n",
    "    complex_corelation = get_one_file_complex_corelation(text, title)\n",
    "    if not os.path.isfile(text) and len(text) < mini_length: return None\n",
    "    elif have_main_point(complex_corelation, plot=False)[0]:\n",
    "        sentences = get_text_sentence(text)\n",
    "        main_sentences = get_main_sentene_with_theme_corelation(sentences, complex_corelation)\n",
    "        print('main sentences: {}'.format(main_sentences))\n",
    "        text_content = get_text_content(text)\n",
    "        summary = change_sentences_to_complete_sentences(main_sentences, text_content)\n",
    "        summary = \"\".join(summary)\n",
    "        return summary\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_sentences_to_complete_sentences(sub_sentences, text):\n",
    "    entire_sentences = [find_complete_sentence(sub_str, text) for sub_str in sub_sentences]\n",
    "    entire_sentences = distinct(entire_sentences)\n",
    "    return entire_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anaylsis No-Linear Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text_corelation(text):\n",
    "    text_sentences = get_text_sentence(text)\n",
    "    distance_map = get_all_sentences_distance(text_sentences)\n",
    "    distance_sentence_pair = [(string, distance_map[string]) for string in text_sentences]\n",
    "    corelation = [1 - d for _, d in distance_sentence_pair]\n",
    "    segments_with_index = [index_word for index_word in enumerate(text_sentences)]\n",
    "    return segments_with_index, corelation, distance_sentence_pair\n",
    "\n",
    "\n",
    "def get_one_file_complex_corelation(text, title):\n",
    "#    print(\"{} {}\".format(text[:50], title))\n",
    "        \n",
    "    corelations = get_text_corelation(text)\n",
    "    sentences = [s for s, d in corelations[2]]\n",
    "    title_distance = get_title_distance(title, sentences)\n",
    "    title_corelations = softmax([1 - d for _, d in title_distance])\n",
    "    content_corelations = softmax(corelations[1])\n",
    "    complex_corelation = get_complex_corelation(title_corelations, content_corelations)\n",
    "    #plot_corelation(complex_corelation[0])\n",
    "    return complex_corelation\n",
    "\n",
    "def get_summary_with_nolinear(text, title, fit_length):\n",
    "    complex_corelation = get_one_file_complex_corelation(text, title)\n",
    "    sentences = get_text_sentence(text)\n",
    "    complete_no_linear = get_complete_sentences_with_corelations(\n",
    "        sentences, complex_corelation, get_text_content(text, escape_english=False))\n",
    "    \n",
    "    complete_no_linear = list(complete_no_linear)\n",
    "    \n",
    "    def f(array, sentences):\n",
    "        total_words_length = len(\"\".join(sentences))\n",
    "        array = list(filter(lambda x: x is not None, array))\n",
    "        content_ratio = len(array)/len(sentences)\n",
    "        result = np.mean(array) * (1.05 ** (np.log(total_words_length * content_ratio + 1)))\n",
    "        return result if not np.isnan(result) else -1\n",
    "\n",
    "    def get_merged_corelation(single_nolinear_corelations, sentences):\n",
    "        merged_corelation = f(single_nolinear_corelations, sentences)\n",
    "        return merged_corelation\n",
    "\n",
    "    def get_sentence_and_merged_corelation(subsentences, corelation):\n",
    "        return (\" \".join(subsentences), corelation)\n",
    "\n",
    "    completed_sentences_with_corelations = []\n",
    "    all_corelations = [c for s, c in complete_no_linear]\n",
    "#    _25th_all_corelations = np.\n",
    "    \n",
    "    for s, c in complete_no_linear:\n",
    "        merged_corelation = get_merged_corelation(c, s)\n",
    "        single_completed_sentence_with_corelation = get_sentence_and_merged_corelation(s, merged_corelation)\n",
    "        completed_sentences_with_corelations.append(single_completed_sentence_with_corelation)\n",
    "\n",
    "#    _25_percentile = np.percentile([c for s, c in completed_sentences_with_corelations], 25)\n",
    "#    _60_percentile = np.percentile([c for s, c in completed_sentences_with_corelations], 60)\n",
    "    corelations = [c for s, c in completed_sentences_with_corelations]\n",
    "    corelations = [x if not np.isnan(x) else -1 for x in corelations]\n",
    "    sentences = [s for s, c in completed_sentences_with_corelations]\n",
    "\n",
    "    top_corelations = top_n(corelations, sentences, fit_length)\n",
    "\n",
    "    total_sentence = []\n",
    "    total_length = 0\n",
    "    min_single_length = 3\n",
    "    for string, corelations in zip(sentences, corelations):\n",
    "        if corelations in top_corelations:\n",
    "            if len(string) >= min_single_length:\n",
    "                total_length += len(string)\n",
    "                total_sentence.append(string)\n",
    "    \n",
    "    return \"。\".join(total_sentence)\n",
    "\n",
    "def top_n(corelations, sentences, fit_length):\n",
    "    sorted_array_with_sentences = sorted(zip(corelations, sentences), key=lambda x: x[0], reverse=True)\n",
    "    max_corelations = []\n",
    "    length = 0\n",
    "    for c, s in sorted_array_with_sentences:\n",
    "        length += len(s)\n",
    "        max_corelations.append(c)\n",
    "        if length >= fit_length: break\n",
    "    return max_corelations\n",
    "        \n",
    "def get_suitable_length_summary(text, title, fit_length):\n",
    "    summary = get_summary_with_nolinear(text, title, fit_length)\n",
    "    return summary\n",
    "    \n",
    "def readable_summary(text, title):\n",
    "    fit_length = get_fit_length(len(get_text_content(text)))\n",
    "    return title + \": \" + get_suitable_length_summary(text, title, fit_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_file_path = 'experiment/error_analysis.txt'\n",
    "title = '“汉语桥”德国大区预选赛落幕'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_text_content(target_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“汉语桥”德国大区预选赛落幕: 第十六届“汉语桥”世界大学生中文比赛德国大区决赛日前在德国法兰克福市落幕。今年比赛的主题是“梦想点亮未来” 来自德国各地孔子学院和大学院系的12位选手进行了近5个小时的激烈角逐。最终 来自慕尼黑工业大学的何本德同学获得大赛一等奖 并将代表德国参加在中国举行的全球总决赛。此次德国赛区的比赛由中国驻德国大使馆教育处主办、法兰克福大学孔子学院承办\n"
     ]
    }
   ],
   "source": [
    "summary = readable_summary(target_file_path, title)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = 'data_preprocess/updated_news/sqlResult_1262716_0524.csv'\n",
    "r = csv.reader(open(csv_file))\n",
    "lines = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = 'experiment/test_if_fit_summary_with_artifical.csv'\n",
    "r = csv.reader(open(csv_file))\n",
    "lines = [l for l in r]\n",
    "for line in lines[1:]:\n",
    "    title = line[1]\n",
    "    content = line[2]\n",
    "    fit = test_if_one_file_fit_summary(content, title)\n",
    "    line[4] = fit[1]\n",
    "    \n",
    "writer = csv.writer(open(csv_file, 'w', newline=''))\n",
    "writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit_to_summary(complex_corelation)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'experiment/fit_summary_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_from_my_file(file, mark):\n",
    "    lines = [line for line in open(file).readlines() if line.startswith(mark)]\n",
    "    ids = [int(line.strip()[len(mark):]) for line in lines]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDs = get_num_from_my_file(file_path, 'ID: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fits = get_num_from_my_file(file_path, 'fit:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ids) == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kouminquan/anaconda/envs/env-3/lib/python3.4/site-packages/ipykernel/__main__.py:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Users/kouminquan/anaconda/envs/env-3/lib/python3.4/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/kouminquan/anaconda/envs/env-3/lib/python3.4/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "test_file = 'test_summary_0619_02.csv'\n",
    "csv_file = 'data_preprocess/updated_news/sqlResult_1262716_0524.csv'\n",
    "contents = pd.read_csv(csv_file)\n",
    "contents = contents.iterrows()\n",
    "differents = []\n",
    "test = []\n",
    "length = 50\n",
    "with open(test_file, 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerow(['id', 'title', 'content', 'description', 'score'])\n",
    "    for C in contents:\n",
    "        if length <= 0: break\n",
    "        ID, content, title = C[1][0], C[1][4], C[1][2]\n",
    "        if random.random() < 0.7: continue\n",
    "        fit, var = test_if_one_file_fit_summary(content, title)\n",
    "        if fit:\n",
    "            try:\n",
    "                summary = readable_summary(content, title)\n",
    "            except IndexError:\n",
    "                print('content: {}'.format(content))\n",
    "                print('title: {}'.format(title))\n",
    "            spamwriter.writerow([ID, title, content, summary, ''])\n",
    "            length -= 1\n",
    "            print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, -1)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_if_one_file_fit_summary(\"2017年05月24日 08:07\", \"中国052D厦门舰新照！将入役东海直面日本海自\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = '黄小蕾控诉迪士尼工作人员故意刁难 工作态度冷漠'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_if_one_file_fit_summary(content, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "differents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter[0] / counter[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(fits, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(fits, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold == 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold == 1.5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_nagative = 7\n",
    "false_positive = 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
